<!DOCTYPE html>
<html lang="en">
<head>
  <title>逻辑回归(分类) - Pinvon&#39;s Blog</title>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
  <meta name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
  <meta name="author" content="Pinvon" />
  <meta name="description" content="&lt;TODO: insert your description here&gt;" />
  <meta name="keywords" content="机器学习实战-读书笔记" />
  <style>
   #ds-thread #ds-reset .ds-comment-body p {color: #ffffff;}
   #ds-thread #ds-reset .ds-comment-body p a {color: #ff0;}
   #ds-thread #ds-reset .ds-comment-body p a:hover {color: #0ff;}
   #disqus_thread a {color: #00ffff;}
  </style>
  <link rel="stylesheet" href="http://pinvondev.github.io/media/css/main.css" type="text/css"/>
  <link rel="stylesheet" href="http://pinvondev.github.io/media/css/comment.css" type="text/css"/>
</head>

  <body><div class="container">
<div>
  <header class="masthead">
    <h1 class="masthead-title"><a href="http://pinvondev.github.io/">Pinvon&#39;s Blog</a></h1>
    <p>所见, 所闻, 所思, 所想</p>
    <nav class="site-nav">
      <div class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" fill="#ffff00"/>
          <path d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" fill="#ffff00"/>
          <path d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" fill="#ffff00"/>
        </svg>
      </div>
      <ul class="trigger">
        <li><a href="http://pinvondev.github.io/years/">Years</a></li>
        <li><a href="http://pinvondev.github.io/authors/">Authors</a></li>
        <li><a href="http://pinvondev.github.io/tags/">Tags</a></li>
        <li><a href="http://pinvondev.github.io/about/">About</a></li>
        <li><a href="https://github.com/pinvondev">Github</a></li>
        <li><a href="http://pinvondev.github.io/rss.xml">RSS</a></li>
      </ul>
    </nav>
    <form method="get" id="searchform" action="http://www.google.com/search">
      <input type="text" class="field" name="q" id="s" placeholder="Search">
      <input type="hidden" name="as_sitesearch" value="pinvondev.github.io">
    </form>
  </header>
</div>

<div>
<div class="post">
<h1 class="title">逻辑回归(分类)</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org4e0e6e7">基于逻辑回归和Sigmoid函数的分类</a>
<ul>
<li><a href="#org2012d16">逻辑回归</a></li>
<li><a href="#orged98f50">Sigmoid函数</a></li>
</ul>
</li>
<li><a href="#org67ccd3b">基于最优化方法的最佳回归系数确定</a>
<ul>
<li><a href="#org15ae3fe">最优化算法: 梯度上升法</a></li>
<li><a href="#orgee4fd70">训练算法: 使用梯度上升找到最佳参数</a></li>
<li><a href="#org2184781">分析数据: 画出决策边界</a></li>
<li><a href="#org5ccc006">训练算法: 随机梯度上升</a></li>
</ul>
</li>
<li><a href="#orgdd71ea2">小结</a></li>
</ul>
</div>
</div>

<div id="outline-container-org4e0e6e7" class="outline-2">
<h2 id="org4e0e6e7">基于逻辑回归和Sigmoid函数的分类</h2>
<div class="outline-text-2" id="text-org4e0e6e7">
</div>
<div id="outline-container-org2012d16" class="outline-3">
<h3 id="org2012d16">逻辑回归</h3>
<div class="outline-text-3" id="text-org2012d16">
<p>
在二维情况下, 假设有一些点, 用一条直线对这些点进行拟合, 这个拟合过程就叫 <code>回归</code>.
</p>

<p>
逻辑回归思想: 根据已有数据, 对分类边界线建立回归公式, 进行分类. 回归的意思是要找到最好的拟合参数集(即最佳的直线). 一般是使用最优化算法来找参数.
</p>

<p>
优点: 计算代价不高, 易于理解和实现.
缺点: 容易欠拟合, 分类精度可能不高.
</p>
</div>
</div>

<div id="outline-container-orged98f50" class="outline-3">
<h3 id="orged98f50">Sigmoid函数</h3>
<div class="outline-text-3" id="text-orged98f50">
<p>
利用Logistic回归进行分类, 主要是要找到一个函数, 能接受所有的输入, 然后预测类别. 这个函数就是Sigmoid函数.
</p>

<p>
假设有 \(n\) 个特征, 将这 \(n\) 个特征与对应的 \(n\) 个系数相乘并累加, 然后将累加后得到的值作为输入传递给sigmoid函数, 就可以对结果进行预测. 即: \(z=w_0x_0 + w_1x_1 + \cdots + w_nx_n\) . 再将 \(z\) 代入Sigmoid函数.
</p>

<p>
Sigmoid函数图象是个S型, 在  \(x=0\) 是, 值为0.5, \(x\) 越大, 越接近1, 越小, 越接近-1.
</p>

<p>
\[\sigma(z) = \frac{1}{1+e^{-z}}\]
</p>
</div>
</div>
</div>

<div id="outline-container-org67ccd3b" class="outline-2">
<h2 id="org67ccd3b">基于最优化方法的最佳回归系数确定</h2>
<div class="outline-text-2" id="text-org67ccd3b">
<p>
Sigmoid函数的输入记为 \(z\) , 且 \(z=w_0x_0 + w_1x_1 + \cdots + w_nx_n\) , 也可写成 \(z=\pmb w^T \pmb x\) .
</p>

<p>
向量 \(\pmb w\) 就是要找的最佳参数.
</p>

<p>
因此, 预测问题转化为寻找最佳参数的问题.
</p>
</div>

<div id="outline-container-org15ae3fe" class="outline-3">
<h3 id="org15ae3fe">最优化算法: 梯度上升法</h3>
<div class="outline-text-3" id="text-org15ae3fe">
<p>
梯度上升法的思想: 要找到某函数的最大值, 最好的方法就是沿着该函数的梯度方向探寻. 梯度的计算公式如下:
\[\triangledown f(x, y) = \Big( \frac{\partial f(x,y)}{\partial x}, \frac{\partial f(x,y)}{\partial y} \Big)\]
意思是要沿 \(x\) 的方向移动 \(\frac{\partial f(x,y)}{\partial x}\), 沿 \(y\) 的方向移动 \(\frac{\partial f(x,y)}{\partial y}\)
</p>

<p>
梯度上升算法中, 每到达一个点后, 都需要重新计算移动的方向. 这样算子总是能保证我们能选取到最佳的移动方向.
</p>

<p>
<code>以爬山为例, 对梯度上升算法做个比喻, 你从山脚开始往上爬, 如果想用最快的速度到达山顶, 则每一步都要沿着最陡的方向(即斜率最大的方向)往上爬.</code>
</p>

<p>
梯度只是移动的方向, 而不是移动量, 记每次移动的步长为 \(\alpha\) , 每走一步都要重新计算下一步移动的方向, 所以梯度算法的更新公式为 \(w := w + \alpha \triangledown_w f(w)\)
</p>

<p>
梯度上升算法与梯度下降算法原理一样, 只是一个算最大值, 一个算最小值.
</p>
</div>
</div>

<div id="outline-container-orgee4fd70" class="outline-3">
<h3 id="orgee4fd70">训练算法: 使用梯度上升找到最佳参数</h3>
<div class="outline-text-3" id="text-orgee4fd70">
<p>
场景: 100个样本点, 每个点包含两个特征. 使用梯度上升法找到最佳回归系数.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #FF1493;">def</span> <span style="color: #87D700;">loadDataSet</span>():
    <span style="color: #FF8C00;">dataMat</span> = [] <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#21015;&#34920;&#30340;&#21015;&#34920;, &#27599;&#20010;&#23567;&#21015;&#34920;&#21253;&#21547;&#19977;&#20010;&#20803;&#32032;[&#22238;&#24402;&#31995;&#25968;, &#29305;&#24449;1, &#29305;&#24449;2]</span>
    <span style="color: #FF8C00;">labelMat</span> = [] <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#27599;&#20010;&#20803;&#32032;&#37117;&#26159;&#20998;&#31867;&#32467;&#26524;</span>
    <span style="color: #FF8C00;">fr</span> = <span style="color: #FF1493;">open</span>(<span style="color: #CDC673;">'testSet.txt'</span>) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">testSet.txt&#30340;&#26684;&#24335;: &#29305;&#24449;1, &#29305;&#24449;2, &#20998;&#31867;&#32467;&#26524;</span>
    <span style="color: #FF1493;">for</span> line <span style="color: #FF1493;">in</span> fr.readlines():
        <span style="color: #FF8C00;">lineArr</span> = line.strip().split()
        dataMat.append([1.0, <span style="color: #FF1493;">float</span>(lineArr[0]), <span style="color: #FF1493;">float</span>(lineArr[1])]) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#22238;&#24402;&#31995;&#25968;&#21021;&#22987;&#21270;&#20026;1.0</span>
        labelMat.append(<span style="color: #FF1493;">int</span>(lineArr[2]))
    <span style="color: #FF1493;">return</span> dataMat, labelMat

<span style="color: #FF1493;">def</span> <span style="color: #87D700;">sigmoid</span>(inX):
    <span style="color: #FF1493;">return</span> 1.0/(1+exp(-inX))

<span style="color: #FF1493;">def</span> <span style="color: #87D700;">gradAscent</span>(dataMatIn, classLabels): <span style="color: #8B8878;"># </span><span style="color: #8B8878;">dataMatlin: 2&#32500;NumPy&#25968;&#32452;, &#19968;&#34892;&#23545;&#24212;&#19968;&#26465;&#25968;&#25454;&#30340;&#29305;&#24449;; classLabels: &#31867;&#21035;</span>
    <span style="color: #FF8C00;">dataMatrix</span> = mat(dataMatIn) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#36716;&#25442;&#25104;NumPy&#30697;&#38453;</span>
    <span style="color: #FF8C00;">labelMat</span> = mat(classLabels).transpose() <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#23558;&#34892;&#21521;&#37327;&#36716;&#25442;&#25104;&#21015;&#21521;&#37327;(&#36716;&#32622;)</span>
    <span style="color: #FF8C00;">m</span>, <span style="color: #FF8C00;">n</span> = shape(dataMatrix) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#35745;&#31639;&#30697;&#38453;&#30340;&#34892;&#25968;&#21644;&#21015;&#25968;</span>
    <span style="color: #FF8C00;">alpha</span> = 0.001 <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#27493;&#38271;</span>
    <span style="color: #FF8C00;">maxCycles</span> = 500 <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#36845;&#20195;&#27425;&#25968;</span>
    <span style="color: #FF8C00;">weights</span> = ones((n, 1)) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">n&#20010;&#31995;&#25968;, &#27599;&#20010;&#37117;&#21021;&#22987;&#21270;&#20026;1</span>
    <span style="color: #FF1493;">for</span> k <span style="color: #FF1493;">in</span> <span style="color: #FF1493;">range</span>(maxCycles):
        <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#35745;&#31639;&#30495;&#23454;&#31867;&#21035;&#19982;&#39044;&#27979;&#31867;&#21035;&#30340;&#24046;&#20540;</span>
        <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#20877;&#26681;&#25454;&#24046;&#20540;&#30340;&#26041;&#21521;&#35843;&#25972;&#22238;&#24402;&#31995;&#25968;</span>
        <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#22914;&#26524;&#24046;&#20540;&#20026;&#27491;, &#21017;&#32487;&#32493;&#24448;&#21069;&#36208;&#19968;&#27493;, &#22914;&#26524;&#24046;&#20540;&#20026;&#36127;, &#21017;&#24448;&#22238;&#36208;&#19968;&#27493;</span>
        <span style="color: #FF8C00;">h</span> = sigmoid(dataMatrix * weights) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">h&#26159;&#19968;&#20010;&#21015;&#21521;&#37327;, &#20803;&#32032;&#20010;&#25968;&#20026;&#26679;&#26412;&#20010;&#25968;</span>
        <span style="color: #FF8C00;">error</span> = (labelMat - h)
        <span style="color: #FF8C00;">weights</span> = weights + alpha * dataMatrix.transpose() * error
    <span style="color: #FF1493;">return</span> weights
</pre>
</div>

<p>
梯度上升算法的返回值是一个列向量, 行数与特征数量相同, 表示每一个特征所对应的回归系数.
</p>
</div>
</div>

<div id="outline-container-org2184781" class="outline-3">
<h3 id="org2184781">分析数据: 画出决策边界</h3>
<div class="outline-text-3" id="text-org2184781">
<p>
上面的代码解出了一组回归系数, 但不一定是最佳系数, 如何找出最佳的回归系数呢?
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FF1493;">def</span> <span style="color: #87D700;">plotBestFit</span>(wei): <span style="color: #8B8878;"># </span><span style="color: #8B8878;">wei&#26159;&#19968;&#32500;&#30697;&#38453;</span>
    <span style="color: #FF1493;">import</span> matplotlib.pyplot <span style="color: #FF1493;">as</span> plt
    <span style="color: #FF8C00;">weights</span> = wei.getA() <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#23558;&#30697;&#38453;&#36716;&#25442;&#20026;&#25968;&#32452;</span>
    <span style="color: #FF8C00;">dataMat</span>, <span style="color: #FF8C00;">labelMat</span> = loadDataSet() 
    <span style="color: #FF8C00;">dataArr</span> = array(dataMat) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#23558;&#21015;&#34920;[&#31995;&#25968;, &#29305;&#24449;1, &#29305;&#24449;2]&#36716;&#25442;&#20026;&#25968;&#32452;</span>
    <span style="color: #FF8C00;">n</span> = shape(dataArr)[0] <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#33719;&#24471;&#30697;&#38453;&#30340;&#34892;&#25968;</span>
    <span style="color: #FF8C00;">xcord1</span> = []; <span style="color: #FF8C00;">ycord1</span> = []
    <span style="color: #FF8C00;">xcord2</span> = []; <span style="color: #FF8C00;">ycord2</span> = []
    <span style="color: #FF1493;">for</span> i <span style="color: #FF1493;">in</span> <span style="color: #FF1493;">range</span>(n):
        <span style="color: #FF1493;">if</span> <span style="color: #FF1493;">int</span>(labelMat[i]) == 1: <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#23558;&#20998;&#31867;&#32467;&#26524;&#20026;1&#21644;0&#30340;&#20998;&#21035;&#23384;&#20648;</span>
            xcord1.append(dataArr[i, 1]); ycord1.append(dataArr[i, 2])
        <span style="color: #FF1493;">else</span>:
            xcord2.append(dataArr[i, 1]); ycord2.append(dataArr[i, 2])
    <span style="color: #FF8C00;">fig</span> = plt.figure() <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#29983;&#25104;&#19968;&#20010;&#22270;</span>
    <span style="color: #FF8C00;">ax</span> = fig.add_subplot(111) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#28155;&#21152;&#32534;&#21495;&#20026;111&#30340;&#23376;&#22270;</span>
    ax.scatter(xcord1, ycord1, s=30, c=<span style="color: #CDC673;">'red'</span>, marker=<span style="color: #CDC673;">'s'</span>)
    ax.scatter(xcord2, ycord2, s=30, c=<span style="color: #CDC673;">'green'</span>)
    <span style="color: #FF8C00;">x</span> = arange(-3.0, 3.0, 0.1) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#20197;-3.0&#20026;&#36215;&#28857;, 3.0&#20026;&#32456;&#28857;, &#27493;&#38271;&#20026;0.1</span>
    <span style="color: #FF8C00;">y</span> = (-weights[0] - weights[1]*x)/weights[2]
    ax.plot(x, y)
    pl.xlabel(<span style="color: #CDC673;">'X1'</span>); plt.ylabel(<span style="color: #CDC673;">'X2'</span>);
    plt.show()
</pre>
</div>
</div>
</div>

<div id="outline-container-org5ccc006" class="outline-3">
<h3 id="org5ccc006">训练算法: 随机梯度上升</h3>
<div class="outline-text-3" id="text-org5ccc006">
<p>
使用梯度上升算法, 在每次更新回归系数时, 都要遍历整个数据集. 如果有数十亿样本和成千上万的特征, 那么该方法的计算复杂度就太高了.
</p>

<p>
随机梯度上升算法可以改善这个问题, 它每次仅用一个样本点来计算误差并更新回归系数, 这样通过多次迭代, 每次都随机选择不同的样本, 最终使回归系数趋于收敛.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #FF1493;">def</span> <span style="color: #87D700;">stocGradAscent0</span>(dataMatrix, classLabels):
    <span style="color: #FF8C00;">m</span>, <span style="color: #FF8C00;">n</span> = shape(dataMatrix)
    <span style="color: #FF8C00;">alpha</span> = 0.01
    <span style="color: #FF8C00;">weights</span> = ones(n)
    <span style="color: #FF1493;">for</span> i <span style="color: #FF1493;">in</span> <span style="color: #FF1493;">range</span>(m):
        <span style="color: #FF8C00;">h</span> = sigmoid(<span style="color: #FF1493;">sum</span>(dataMatrix[i] * weights))
        <span style="color: #FF8C00;">error</span> = classLabels[i] - h
        <span style="color: #FF8C00;">weights</span> = weights + alpha * error * dataMatrix[i]
    <span style="color: #FF1493;">return</span> weights
</pre>
</div>
<p>
这段代码与之前的梯度上升算法主要有两点不同:
</p>
<ol class="org-ol">
<li>随机梯度上升算法的变量h是一个数字, 而梯度上升算法中是一个向量.</li>
<li>随机梯度上升算法没有进行矩阵转换, 而梯度上升算法则将矩阵转换成NumPy数组.</li>
</ol>

<p>
如果该代码报错: TypeError: 'numpy.float64' object cannot be interpreted as an integer. 则在调用该函数时, 第一个参数使用array()转一下.
</p>

<p>
随机梯度上升算法中, 一次循环只用了其中一个样本, 来对回归系数进行更新(第 \(i\) 次循环就用第 \(i\) 个样本来更新系数). 而在梯度上升算法中, 每次循环, 都使用了所有的样本来对回归系数进行更新.
</p>

<p>
在这个代码中, 样本有多少, 就循环了多少次, 所以如果样本不多, 则未必系数是较好的, 但是在样本足够多的情况下, 可以在较短的时间内(相对梯度上升算法而言)达到稳定值.
</p>

<p>
一个判断优化算法优劣的可靠方法是看它是否收敛, 也就是说参数是否达到了稳定值, 是否还会不断地变化.
</p>

<p>
但是, 在大的波动停止之后, 还有一些小的周期性波动. 产生这种现象的原因是存在一些不能正确分类的样本点(数据集并非线性可分), 在每次迭代时会引发系数的剧烈改变. 因此, 对随机梯度上升算法进行改进, 避免来回波动.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #FF1493;">def</span> <span style="color: #87D700;">stocGradAscent1</span>(dataMatrix, classLabels, numIter=150):
    <span style="color: #FF8C00;">m</span>, <span style="color: #FF8C00;">n</span> = shape(dataMatrix)
    <span style="color: #FF8C00;">weights</span> = ones(n)
    <span style="color: #FF1493;">for</span> j <span style="color: #FF1493;">in</span> <span style="color: #FF1493;">range</span>(numIter):
        <span style="color: #FF8C00;">dataIndex</span> = <span style="color: #FF1493;">range</span>(m) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">0-m</span>
        <span style="color: #FF1493;">for</span> i <span style="color: #FF1493;">in</span> <span style="color: #FF1493;">range</span>(m):
            <span style="color: #FF8C00;">alpha</span> = 4 / (1.0 + j + i) + 0.01 <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#38543;&#30528;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;, &#27493;&#38271;&#19981;&#26029;&#20943;&#23567;, &#20294;&#19981;&#20250;&#20943;&#21040;0, &#22240;&#20026;&#23384;&#22312;&#24120;&#25968;&#39033;</span>
            <span style="color: #FF8C00;">randIndex</span> = <span style="color: #FF1493;">int</span>(random.uniform(0, <span style="color: #FF1493;">len</span>(dataIndex))) <span style="color: #8B8878;"># </span><span style="color: #8B8878;">&#38543;&#26426;&#36873;&#21462;&#26679;&#26412;&#26356;&#26032;&#22238;&#24402;&#31995;&#25968;</span>
            <span style="color: #FF8C00;">h</span> = sigmoid(<span style="color: #FF1493;">sum</span>(dataMatrix[randIndex] * weights))
            <span style="color: #FF8C00;">error</span> = classLabels[randIndex] - h
            <span style="color: #FF8C00;">weights</span> = weights + alpha * error * dataMatrix[randIndex]
            <span style="color: #FF1493;">del</span>(dataIndex[randIndex])
    <span style="color: #FF1493;">return</span> weights
</pre>
</div>
<p>
代码中, <code>alpha</code> 会随着迭代的次数增加而不断减小, 这样可以缓解后期的波动. 然后, 随机选取样本来更新回归系数.
</p>

<p>
随机梯度上升算法可以通过更小的计算量, 来得到与梯度上升算法差不多的效果.
</p>
</div>
</div>
</div>

<div id="outline-container-orgdd71ea2" class="outline-2">
<h2 id="orgdd71ea2">小结</h2>
<div class="outline-text-2" id="text-orgdd71ea2">
<p>
逻辑回归的目的是寻找一个 <code>非线性函数sigmoid的最佳拟合参数</code> , 求解过程可以由最优化算法来完成.
</p>

<p>
随机梯度上升算法与梯度上升算法的效果相当, 但占用更少的计算资源. 此外, 随机梯度上升是一个在线算法, 它可以在新数据到来时就完成参数更新, 而不需要重新读取整个数据集来进行批处理运算.
</p>

<p>
机器学习的一个重要问题是如何处理缺失数据, 这个问题没有标准答案, 取决于实际中的需要.
</p>
</div>
</div>

</div>
</div>
<div>
        <div class="post-meta">
            <span title="post date" class="post-info">2018-01-05</span>
            <span title="last modification date" class="post-info">2018-03-23</span>
            <span title="tags" class="post-info">:<a href="http://pinvondev.github.io/tags/机器学习实战-读书笔记">机器学习实战-读书笔记</a>:</span>
            <span title="author" class="post-info"><a href="mailto:pinvon@Inspiron">Pinvon</a></span>
        </div>
    <script src="http://pinvondev.github.io/media/js/jquery-2.1.3.min.js"></script>
        <section>
            <h1>Comments</h1>
            <div id="comment-wrap">
                    <a class="disqus_label">使用 Disqus 评论</a>
    </ul>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
         //var disqus_developer = 1;
         var preempt_signal=false;
         var disqus_identifier = "/blog/2018/01/05/逻辑回归";
         var disqus_url = "http://pinvondev.github.io/blog/2018/01/05/逻辑回归";
         var disqus_shortname = 'pinvon';
         /* * * DON'T EDIT BELOW THIS LINE * * */
         (function() {
             var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
             dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
             (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
             $('#disqus_thread').css('display','none');
         })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    <script>
     /* comments */
     var ego_disqus_thread=$('#disqus_thread');
     var ego_ds_label=$('.ds-thread');
     $('.disqus_label').click(function(){
         ego_disqus_thread.show();
         ego_ds_label.hide();
     });
     $('.ds-label').click(function(){
         ego_disqus_thread.hide();
         ego_ds_label.show();
     });
    </script>
        </section>
    <script src="http://pinvondev.github.io/media/js/main.js"></script>
    <div class="footer">
        <p>Generated by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.x(<a href="http://orgmode.org">Org mode</a> 9.x)</p>
        <p>
            Copyright &copy; 2014 - <span id="footerYear"></span> <a href="mailto:pinvon@Inspiron">Pinvon</a>
            &nbsp;&nbsp;-&nbsp;&nbsp;
            Powered by <a href="https://github.com/emacs-china/ego" target="_blank">EGO</a><br/>
            <a href="http://creativecommons.org/licenses/by-sa/3.0/" rel="license"><img src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png" style="border-width:0" alt="Creative Commons License" class="center"></a>
            <script type="text/javascript">document.getElementById("footerYear").innerHTML = (new Date()).getFullYear();</script>
        </p>
    </div>
            </div>
            <script type="text/javascript"
                    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
            </script>
            <script type="text/x-mathjax-config">
             MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
            </script>
</div>

  </div></body>
</html>
