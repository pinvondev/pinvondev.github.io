#+TITLE:       词向量
#+AUTHOR:      Pinvon
#+EMAIL:       pinvon@Inspiron
#+DATE:        2018-04-11 三
#+URI:         /blog/%y/%m/%d/k-means-python
#+KEYWORDS:    <TODO: insert your keywords here>
#+TAGS:        机器学习
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:t \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: <TODO: insert your description here>

* 什么是词向量

要将自然语言交给机器学习中的算法来处理, 首先需要将语言数字化.

向量是我们把自然界的东西抽象出来交给机器处理的东西, 可以说, 向量是人对机器输入的主要方式.

词向量把语言中的词进行数字化, 即把一个词表示成一个向量.

表示方法主要有两种, 分别是 =one-hotrepresentation= 和 =distributedRepresentation=.

** one-hotrepresentation

该方法用一个很长的向量来表示一个词, 向量的长度为词典的大小. 向量中只有一个1, 其他都是0, 1的位置就是该词在词典中的位置. 每个词都是一堆0中的一个1.

如:
'中国'表示为 [0001000...]
'美国'表示为 [0000010...]

如果对词向量采用稀疏方式进行存储, 则很简单, 就是给每个词分配1个ID, 如中国可以分配3, 美国分配5(从0开始数).

如果要编程实现, 可以用hash表给每个词分配一个编号, 再加上熵, SVM, CRF等算法, 就可以完成很多NLP领域的任务.

*** 缺点

- 维数灾难
- 不能很好地刻画词与词之间的相似性: 任意两个词之间都是孤立的, 光从两个向量中看不出两个词是否有关系, 即使是'丈夫'和'老公'这两个同义词, 是完全孤立的.

** distributedRepresentation

这种表示法的思想: 用一个普通的向量表示一个词, 这种向量形式为[0.792, -0.177, -0.107, ...].

一个词要如何表示成一个向量是需要训练的, 常用的训练方法是word2vec. 另外, 每个词在不同的语料库和不同的训练方法下, 得到的词向量可能是不一样的.

这种词向量的维数一般不会很高, 所以就没有维数灾难的问题了.

如果是较好的训练算法, 可以得到如下结果: 将所有向量放在一起形成一个词向量空间, 每个向量都是该空间中的一个点, 在这个空间上的词向量之间的距离度量也可以表示对应两个词之间的距离, 即这两个词之间的语法和语义上的相似性.
