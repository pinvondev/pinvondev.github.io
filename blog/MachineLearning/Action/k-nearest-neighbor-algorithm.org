#+TITLE:       k-近邻算法
#+AUTHOR:      Pinvon
#+EMAIL:       pinvon@Inspiron
#+DATE:        2018-01-01 一
#+URI:         /blog/%y/%m/%d/k-近邻算法
#+KEYWORDS:    <TODO: insert your keywords here>
#+TAGS:        《机器学习实战》读书笔记
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:t \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: <TODO: insert your description here>

* k-近邻算法概述

 $k$ -近邻算法是监督学习算法，用于解决分类问题。

对于训练集中的每个数据，其所属分类已知。输入新数据后，根据新数据的特征与训练集中的特征进行比较，得出新数据最接近的分类。而 $k$  这个名字的来源，是因为一般只与前 $k$ 个最相似的数据比较，其中， $k \leq 20$ 。

思考：如果 $k$ 太大，有很多相似度较低的数据也参与了比较，在结果中与相似度较高的数据有同等的权重决定分类结果，会使得结果不准确。当然，也可以为这些相似度低的数据分配较低的权重，但这样也增加了复杂的复杂度。

使用欧氏距离公式来计算距离。然后对距离进行排序，确定前 $k$ 个距离的元素所在的主要分类。

使用数值归一化的情况：假如结果由两个特征决定，由于特征1的数值很大，所以特征1对结果造成的影响很大，而实际上，特征1和特征2的权重是相同的，这时候，需要对特征进行数值归一化。方法： $new=\frac{old-min}{max-min}$ 。

 $k$ -近邻算法是分类数据最简单最有效的方法，它是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。 $k$ -近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。

