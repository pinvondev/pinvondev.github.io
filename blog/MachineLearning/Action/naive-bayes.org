#+TITLE:       基于概率论的分类方法: 朴素贝叶斯
#+AUTHOR:      Pinvon
#+EMAIL:       pinvon@Inspiron
#+DATE:        2018-01-03 三
#+URI:         /blog/%y/%m/%d/基于概率论的分类方法-朴素贝叶斯
#+KEYWORDS:    <TODO: insert your keywords here>
#+TAGS:        机器学习实战-读书笔记
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:t \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: <TODO: insert your description here>

* 概述

朴素贝叶斯是一个分类算法.

$k$ -近邻算法和决策树这两种算法, 都要求分类器做出明确的决策, 指出该数据属于哪个分类. 但是在很多情况下, 答案并不是明确的, 只能猜测属于这个分类的概率.

* 基于贝叶斯决策理论的分类方法

优点: 在数据较少的情况下仍然有效,可以处理多类别问题.
缺点: 对于输入数据的准备方式较为敏感.

有两种概率理论: 贝叶斯概率理论; 频数概率; 
贝叶斯概率理论引入先验知识和逻辑推理来处理不确定命题.
频数概率从数据本身获得结论, 并不考虑逻辑推理及先验知识.

* 条件概率

计算公式1: $P(A \mid B) = \frac{AB}{B}$

还有一种计算条件概率的方法, 称为贝叶斯准则. 贝叶斯准则告诉我们交换条件概率中的条件与结果, 即: 如果已知 $p(B \mid A)$ , 要求 $p(A \mid B)$ , 则可以使用下面的公式:
$$p(A \mid B) = \frac{p(B \mid A)p(A)}{p(B)}$$

* 使用朴素贝叶斯进行文档分类

朴素贝叶斯是贝叶斯分类器的一个扩展, 常用于文档分类. 对于文档, 可以把每个记事的出现或者不出现作为一个特征, 这样得到的特征数目就会跟词汇表中的词目一样多.

一般来说, 如果每个特征需要 $N$ 个样本, 那么对于10个特征, 将需要 $N^10$ 个样本. 如果特征之间相互独立, 那么样本数就可以从 $N^10$ 减少到 $10 \times N$ 个.

朴素贝叶斯的前提是: 特征之间相互独立, 同等重要. 这也是朴素一词的含义.

* 使用python进行文本分类

分类: 侮辱类和非侮辱类.

** 从文本中构建词向量(将单词转化为一组数字)

loadDataSet(). 输出: 分好词的集合, 分类结果(1, 0)
不过书中的程序并没有实现如何分词, 而是为了方便, 直接输入分好了词的数据.

createVocabList(). 输入: 数据集; 输出: dataset中不重复词列表
利用set来实现不重复

setOfWords2Vec(). 输入: 词汇表, 文档; 输出: 文档向量
文档向量中每个元素为1或0, 分别表示词汇表中的单词在输入文档中是否出现.

** 训练算法: 从词向量计算概率

已知: 分类(侮辱类, 非侮辱类); 文档向量

计算: 给定一篇文档, 计算它所属分类.

我们将文档转化成文档向量 $w$ , $w$ 中某个词汇出现则对应元素置为1, 否则对应元素置为0. 在知道 $w$ 的情况下, 计算文档属于分类 $c_i$ 的概率 $p(c_i \mid w)$ . 根据贝叶斯公式, $p(c_i \mid w) = \frac{p(w \mid c_i)p(c_i)}{p(w)}$ .

由于朴素贝叶斯假设各个特征相互独立. 因此,  $p(w \mid c_i) = p(w_0 \mid c_i) \cdot p(w_1) \mid c_i \cdots p(w_n \mid c_i)$ .

而 $p(w_i \mid c_i)$ 是很好计算的.

trainNB0(). 输入: 文档矩阵, 类别; 输出: 两个向量和一个概率
计算每个类别中的文档数目
对每篇训练文档:
	对每个类别:
		如果词条出现在文档中 -> 增加该词条的计数值
		增加所有词条的计数值
	对每个类别:
		对每个词条:
			将该词条的数目除以总词条数目得到条件概率
	返回每个类别的条件概率
